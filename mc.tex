\documentclass[main.tex]{subfiles}
\begin{document}
\chapter{Monte Carlo Event Generators}\label{chapter:MC}
    At particle collider experiments,
    the collisions between incoming particles can lead
    to highly complex final-states with a large number
    of particles. In any collider involving QCD processes,
    the particles we observe are not the constituents
    of the proton, the quarks and gluons. Instead we observe
    hadrons which are the bound states of these partons
    due to the property of confinement at low energy scales.

    In order to describe the entire process
    from the initial energetic collision to the production
    of the lower energy hadrons, it is customary to
    split up the process into stages characterised by
    their kinematic hardness (c.f. Section~\ref{sec:factorisation}).
    The standard tools used
    to simulate these stages of an event are general-purpose
    Monte Carlo event generators, see
    Ref. \cite{Buckley:2011ms} for a review.
    These are indispensable tools that help
    bridge the gap between theoretical predictions
    and experimental measurements with nearly all
    analyses made at the ATLAS and CMS experiments
    involving some form of event generator usage.
    General-purpose event generators
    such as {\Herwig} \cite{Bahr:2008pv,Bellm:2015jjp},
    {\Pythia} \cite{Sjostrand:2006za,Bierlich:2022pfr},
    and {\Sherpa} \cite{Gleisberg:2008ta,Sherpa:2019gpd} all broadly share 
    the same features at a high-level, but each framework
    carries subtle differences. Hence, it is common
    to use many different event generators
    in one analysis to obtain an estimate of the uncertainty
    due to the different approaches in each framework.

    In the following we will motivate the usage of
    Monte Carlo methods and give a brief overview
    of the main stages involved in simulating an event.
    We will give particular attention to the concept of event
    unweighting which is required to compare probabilistically
    generated events to experimental results.

    \section{Monte Carlo integration}\label{sec:MC_integration}
    As seen in Section~\ref{sec:xs}, the task of
    computing partonic cross-sections boils down
    to integrating the matrix elements over the
    final-state phase-space. Whilst it is possible to
    carry out these integrals analytically for the simplest
    cases, the integral quickly becomes intractable when the
    final-state multiplicity becomes large. For an $n$-body
    final-state the dimensionality of the integral scales as $d=3n-4$,
    where momentum conservation and on-shell conditions
    have already been applied to reduce the dimensionality.
    Therefore, instead of carrying out the integrals analytically,
    numerical methods become more attractive. The current
    prevailing method is Monte Carlo integration as the
    integration error has no scaling with the dimensionality of the integral,
    and so can be used for arbitrarily high multiplicity final-states
    with no performance penalty compared to lower multiplicities.
    The purpose of this chapter is to give a brief overview
    of Monte Carlo integration in the context of event
    generation, a more in-depth treatment of the
    specifics of Monte Carlo methods can be found in Ref.
    \cite{James:1980yn}.

    The basic idea of Monte Carlo integration is to approximate
    the integral of a function $f(\mathbf{x})$ by sampling it
    in $d$-dimensional parameter space points $\mathbf{x}$
    \begin{equation}\label{eqn:MC_integration}
        I = \int_{V} \mathrm{d}\mathbf{x} \; f(\mathbf{x}) \approx I_{N} = V \dfrac{1}{N} \sum_{i=1}^{N} f(\mathbf{x}_{i}) = V \langle f \rangle_{\mathbf{x}} \, ,
    \end{equation}
    where the points $\mathbf{x}$ are randomly,
    uniformly sampled in the volume $V$. Given a sample of $N$
    points $\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\} \in V$,
    $\langle f \rangle_{\mathbf{x}}$
    denotes the average of $f$ over these uniformly sampled
    points. The law of large numbers ensures that the
    estimator $I_{N}$ will converge to
    the true value $I$ for $N \rightarrow \infty$. As a
    result of the central limit theorem, the function evaluations
    $f(\mathbf{x}_{i})$ will follow a Gaussian distribution with
    mean $I_{N}$,
    meaning an error estimate on the estimator is given by
    the standard error of the mean
    \begin{equation}\label{eqn:MC_error}
        \sigma_{I} = V \sqrt{\dfrac{\langle f^{2} \rangle_{\mathbf{x}} - \langle f \rangle_{\mathbf{x}}^{2}}{N-1}}
    \end{equation}
    which falls as $1/\sqrt{N}$, with no dependence
    on $d$.

    An algorithm that samples points $\mathbf{x}$ is called
    an integrator. To make use of the random numbers sampled,
    they need to be mapped to quantities that are useful in a simulation
    setting. The most commonly used mapping is to map
    the random numbers to four-momenta. An example of a
    simple integrator is the {\RAMBO} algorithm \cite{Kleiss:1985gy,Platzer:2013esa}
    which samples four-momenta space uniformly in a phase-space
    integration. Since the integrator
    now effectively samples in momentum space, we refer to the points
    generated as phase-space points and so the terms integrator
    and phase-space generator are used interchangeably.

    Whilst Monte Carlo integration is independent of the
    dimensionality of the integral, meaning convergence is
    guaranteed for large enough $N$, there are variance
    reduction methods to more rapidly reduce the error
    on the estimator. These methods aim to reduce the numerator
    in Eq.~(\ref{eqn:MC_error}).

    One such method is importance sampling which
    aims to reduce the variance via a remapping of the
    uniformly sampled variables $\mathbf{x}$
    to a more suitable non-uniform distribution.
    In the context of particle physics,
    this corresponds to using a mapping of random numbers
    to four-momenta that captures the distribution of the
    matrix elements and their divergences, i.e. samples
    more frequently in regions where the matrix element
    is large in magnitude. Integrators inspired by the pole
    structure of multi-parton QCD processes have been
    studied in Refs. \cite{Draggiotis:2000gm,vanHameren:2002tc}.
    
    Another common variance reduction technique is
    stratified sampling which divides the integration
    volume into sub-volumes, or bins. The overall integral
    and variance are the sums of the partial results
    in each bin. The overall variance is minimised when
    the contributions from each bin are equal. This
    is achieved by sampling more from bins where the integral
    is rapidly fluctuating (and not necessarily large),
    and less from bins where the integral has fewer fluctuations.
    
    In practice, state of the art event generators
    combine the concepts of importance sampling and stratified
    sampling with adaptive sampling \cite{Lepage:1977sw,Lepage:2020tgj}
    and multi-channelling methods \cite{Kleiss:1994qy,Ohl:1998jn}
    to achieve faster rates of convergence.

    \section{Event unweighting}\label{sec:unweighting}
    Once a suitable configuration of momenta
    has been found by the integrator, this phase-space point forms the basis of an
    event. The matrix element of interest can be evaluated at
    this phase-space point at the desired order in perturbation
    theory (see Section~\ref{sec:me_generators}).
    Additionally, there will be an associated Jacobian
    from the mapping from random variables to the four-momenta.
    This Jacobian, along with the PDF weights and flux factors,
    are referred to as the phase-space weight, $J_{\mathrm{PS}}$. Taking
    this phase-space weight together
    with the matrix element forms the event weight\footnote{For
    the more sophisticated sampling methods employed in event generators
    there will be additional weights involved.}
    \begin{equation}\label{eqn:event_weight}
        w = |\mathcal{M}|^{2} J_{\mathrm{PS}} \, .
    \end{equation}
    The event weight can be interpreted as the probability
    of the event occurring at an experiment \cite{Plehn:2009nd}.
    By generating a number of events in this manner
    a sample of weighted events is created. Cross-sections
    and distributions can then be calculated by histogramming
    these weights.
    
    The generation of weighted events is generally undesirable
    as many events will have small weights with only a few events
    making sizeable contributions to predictions. Furthermore,
    these small weights still have to be run through expensive detector
    simulations, making the usage of weighted events highly inefficient.
    Therefore, it has become customary to generate events
    with equal relative weight, so-called unweighted events.

    An additional reason to generate unweighted events is that
    experimentalists would like the probability of events occurring
    at colliders to be described only by their frequency and not
    with any additional weights. Since unweighted events have
    equal relative weight, they satisfy this criteria.

    The process of generating unweighted events is known as
    unweighting and is typically carried out using a rejection
    sampling algorithm.
    Events are accepted or rejected by comparing the ratio
    $w / w_{\mathrm{max}}$ to a uniform random number, $r$,
    sampled in the interval $[0, 1]$, where $w_{\mathrm{max}}$ is
    the maximal event weight in the integration volume.
    Events are then accepted if $w / w_{\mathrm{max}} > r$.
    This can be interpreted as converting the event weights
    into a probability of being accepted or rejected as
    small event weights will have a low probability of being
    accepted, whereas the opposite is true for large event weights.
    The rejection algorithm is outlined explicitly in
    Algorithm~\ref{alg:unweighting}.

    \begin{algorithm}[ht]
        \caption{Rejection sampling for unweighting events}\label{alg:unweighting}
        \While{unweighting}{
            generate random phase-space point $\mathbf{x}$\;
            evaluate event weight $w \gets w(\mathbf{x})$\;
            generate uniform random number $r \gets$ Random(0, 1)\;
            \If{$w / w_{\mathrm{max}} > r$}{
                \KwRet $\mathbf{x}$ and $\tilde{w} \gets 1$\;
                % $\tilde{w} \gets \max(1,\; w / w_{\mathrm{max}})$\;
            }
        }
    \end{algorithm}

    From this description, it is clear that the number
    of accepted events will generally be lower than
    the number of total events generated. This is encapsulated
    in the unweighting efficiency
    \begin{equation}\label{eqn:unweighting_efficiency}
        \epsilon = \dfrac{N_{\mathrm{accepted}}}{N_{\mathrm{total}}} \, ,
    \end{equation}
    whose inverse is the average number of trial
    events required to obtain a single unweighted event.
    Unweighting efficiencies for high-multiplicity processes
    are often well below 1\% \cite{Hoche:2019flt,Gao:2020zvv}.
    The development of methods to improve the efficiency are an active
    area of research \cite{Jadach:1999sf,Jadach:2002kn}, wherein the
    authors explore an algorithm that adaptively splits up the integration
    region in order to systematically reduce the ratio of
    the maximum event weight to the average event weight. This leads to
    increased unweighting efficiency when carrying out the rejection sampling
    algorithm.
    These low efficiencies lead to most of the
    yearly event generator CPU budgets of LHC experiments to be spent on
    unweighting events \cite{HSFPhysicsEventGeneratorWG:2020gxw}.

    An alternative to improving the unweighting efficiency
    is to accelerate the process of unweighting itself. This
    was explored in Ref. \cite{Danziger:2021eeg} wherein a
    novel two-stage unweighting procedure employing a surrogate\footnote{The authors of Ref.~\cite{Danziger:2021eeg}
    refer to their model approximating the event weight as a surrogate model, whereas in
    this thesis we have used the term emulator. We will use these two terms interchangeably
    henceforth.}
    model of the exact event weight was used to decrease the amount of
    time spent unweighting events. This idea will be elaborated on
    in Chapter~\ref{chapter:fame3} where we build upon
    Ref. \cite{Danziger:2021eeg} by increasing the fidelity of
    the surrogate model.

    \section{Anatomy of an event}\label{sec:anatomy_event}
    The simulation of an event can be split up into three
    main stages: the hard scattering process, followed by
    the parton shower, and finally hadronisation.
    This is motivated by the factorisation theorem (c.f. Section~\ref{sec:factorisation})
    which separates the high energy regimes from the low
    energy regimes.

    \subsection*{Hard scattering}\label{sec:me_generators}
    The first stage of an event simulation begins with
    the computation of the matrix element. The partonic
    channel of interest sets the multiplicity of the process,
    therefore informing the integrator on how to sample the phase-space
    accordingly.

    As seen in Eq. (\ref{eqn:matrix_element}),
    the exact matrix element is approximated by a perturbative
    series in a coupling parameter. By now it is standard to
    have at least the LO and NLO contributions in this expansion.
    The calculations of
    tree-level \cite{Mangano:2002ea,Krauss:2001iv,Gleisberg:2008fv,Cafarella:2007pc,Alwall:2011uj,Kilian:2007gr}
    and one-loop \cite{Berger:2008sj,Cullen:2011ac,Cullen:2014yla,Bevilacqua:2011xh,Hirschi:2011pa,Alwall:2014hca,Frederix:2018nkq,Campbell:1999ah,Campbell:2021vlt,Badger:2012pg,Cascioli:2011va,Buccioni:2019sur,Actis:2016mpe,Denner:2017wsf}
    matrix elements are now largely fully automated
    in NLO QCD and electroweak corrections.
    Many of the modern matrix element providers are supplied
    with a common generic interface to event generators,
    the Binoth Les Houches Accord \cite{Binoth:2010xt,Alioli:2013nda}.
    For more information on the machinery developed to
    compute matrix elements at tree-level and
    one-loop level required for SM predictions, see Ref. \cite{Campbell:2017hsr}.
    NNLO and N$^{3}$LO QCD corrections have been computed
    for select processes but have yet to reach the
    level of automation of NLO.
    For a review of recent advancements see Refs. \cite{Caola:2022ayt,Campbell:2022qmc}.
    
    Matrix element calculations account for a large
    portion of the total event generation time \cite{Bothmann:2022thx}.
    Reducing the time spent in this stage of the event
    generation therefore represents a sizeable increase
    in the number of events generated given a fixed computing
    budget \cite{HSFPhysicsEventGeneratorWG:2020gxw}.
    This thesis presents a novel technique to
    accelerate matrix element calculations by building
    accurate emulation models from modern machine learning
    techniques.

    \subsection*{Parton shower}\label{sec:parton_showers}
    Fixed-order calculations of matrix elements are limited
    to relatively few final-state particles due to the complexity
    of analytical expressions and their phase-space integrals.
    They are also only accurate at high energy scales. For low
    transverse momentum emissions, large logarithms appearing at
    every order breaks the $\alpha_{s}$ perturbative expansion.
    To describe these kinematic regions, and to evolve from the fixed-order
    calculation to the non-perturbative regime where hadrons
    are formed, parton showers are employed in event generators.

    Parton showers are implemented as algorithms that
    iteratively emit radiation (gluons and quarks in QCD
    parton showers) described by splitting functions.
    By repeating this procedure of emissions, parton
    showers produce a cascade of partons that naturally
    evolves the energy scale from the hard process to
    an infrared scale where non-perturbative effects
    begin to set in. In doing so, parton showers account
    for potentially large logarithms that arise in
    these kinematic regimes at all orders in the coupling
    parameter, this is known as resummation \cite{Buckley:2011ms}.

    In event generators, fixed-order matrix elements
    and parton showers have to be utilised together to describe
    the many different observables at experiments. In order
    to combine the two stages of event generation in a consistent
    manner there are matching \cite{Frixione:2002ik,Frixione:2007vw,Jadach:2015mza}
    and merging \cite{Catani:2001cc,Lonnblad:2011xx}
    schemes.

    \subsection*{Hadronisation}\label{sec:hadronisation}
    The simulation of the formation of hadrons in the low-energy regime
    of an event generator follows phenomenological models,
    which are non-perturbative and consist of many
    parameters fitted to data.
    These hadronisation models require the combination
    of quarks and antiquarks to form colourless hadrons.
    The two main classes of models used to simulate
    hadronisation are the cluster models \cite{Webber:1983if,Winter:2003tt}
    and string models \cite{Andersson:1983ia}.
    For more discussion on hadronisation
    see Ref. \cite{Webber:1999ui}.

    \section{Estimation of theoretical uncertainties}\label{sec:scale_variations}
    The partonic cross-section, $\hat{\sigma}(\mu_{F}, \mu_{R})$, has
    a dependence on both the factorisation scale and the
    renormalisation scale. Although these scales are unphysical,
    and so any observable should not depend on them,
    there is a residual dependence on them in fixed-order
    calculations due to missing higher order terms.

    The most widely adopted approach to estimate the
    theoretical uncertainties associated with these
    missing higher order terms is to carry out a seven point
    scale variation. Typically in a calculation,
    $\mu_{0} = \mu_{F} = \mu_{R}$ where $\mu_{0}$ depends on the specifics
    of the process of interest. Taking the value of the
    partonic cross-section at this scale as the central value,
    the seven variations correspond to multiplying ($\mu_{F}, \, \mu_{R}$) by the factors 
    \begin{equation}\label{eqn:scale_variations}
        S \in \left\{(1, 1), (\sfrac{1}{2}, \sfrac{1}{2}), (2, 2), (\sfrac{1}{2}, 1), (1, \sfrac{1}{2}), (2, 1), (1, 2) \right\} \, ,
    \end{equation}
    and re-evaluating the cross-section at these scales.
    The deviations from the central cross-section value
    forms an estimate of the uncertainty in the perturbatively
    computed cross-section, when compared to the central cross-section value.
    This can be written as
    \begin{equation}
        \hat{\sigma}(\mu_{F}, \mu_{R}) = \hat{\sigma}(\mu_{0}, \mu_{0}) \pm \delta \, ,
    \end{equation}
    where $\delta$ is determined from the scale variations.
    However, there is no single, well agreed upon definition
    of $\delta$ \cite{Cacciari:2011ze}.
    The rationale behind taking this approach is that $\delta$
    is approximately the same order of magnitude as the true missing
    higher order terms.

\end{document}