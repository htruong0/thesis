\documentclass[main.tex]{subfiles}
\begin{document}
\chapter{Monte Carlo Event Generators}\label{chap:MC}
    At a particle collider experiment,
    the collisions between incoming particles can lead
    to highly complex final-states with a large number
    of particles. In a hadron collider, such as the LHC,
    the particles that we observe are not the constituents
    of the proton, the quarks and gluons. Instead we observe
    hadrons which are the bound states of these partons
    due to the property of confinement at low energy scales.

    In order to describe the entire chain of events
    from the initial energetic collision to the production
    of the lower energy hadrons, it is customary to
    split up the process into stages characterised by
    their kinematic regime. The standard tool used
    to simulate these stages of an event are general-purpose
    Monte Carlo event generators (event genrators), see
    Reference \cite{Buckley:2011ms} for a review.
    They are indispensable tools that help
    bridge the gap between theoretical predictions
    and experimental measurements with nearly all
    analyses made at the ATLAS and CMS experiments
    involving some form of event generator usage.

    General-purpose event generators available
    such as Herwig \cite{Bahr:2008pv,Bellm:2015jjp},
    Pythia \cite{Sjostrand:2006za,Bierlich:2022pfr},
    and Sherpa \cite{Gleisberg:2008ta,Sherpa:2019gpd} all broadly share 
    the same features at a high-level overview.

    In the following we will motivate the necessity
    for Monte Carlo methods and give a brief overview
    of the main stages involved in simulating an event.
    We will give particular attention to the concept of event
    unweighting which is required to compare probabilistically
    generated events to experimental results.

    \section{Monte Carlo integration}\label{sec:MC_integration}
    As seen in Section~\ref{sec:xs}, the task of
    computing partonic cross-sections boils down
    to integrating the matrix elements over the
    final-state phase-space. Whilst it is possible to
    carry out these integrals analytically for the simplest
    cases, the integral quickly becomes intractable when the
    final-state multiplicity becomes large. For an $n$-body
    final-state the dimensionality of the integral scales as $d=3n-4$,
    where momentum conservation and on-shell conditions
    have already been applied to reduce the dimensionality.
    Therefore, instead of carrying out the integrals analytically,
    numerical methods become more attractive. The current
    prevailing method is Monte Carlo integration which
    has no scaling with the dimensionality of the integral,
    so can be used for arbitrarily high multiplicity final-states.
    The purpose of this chapter is to give a brief overview
    of event generators, a more indepth treatment of the
    specifics of Monte Carlo methods is given in Reference
    \cite{James:1980yn}.

    The basic idea of Monte Carlo integration is to approximate
    the integral of a function $f(\vec{x})$ by sampling it
    at $d$-dimensional parameter space points $\vec{x}$
    \begin{equation}\label{eqn:MC_integration}
        I = \int_{V} \mathrm{d}^{D}x \; f(\vec{x}) \approx \langle I(f) \rangle_{x} = V \dfrac{1}{N} \sum_{i=1}^{N} f(\vec{x}_{i}) = V \langle f \rangle_{x} \, ,
    \end{equation}
    where the points $\vec{x}$ are randomly (hence Monte Carlo),
    uniformly sampled in the volume $V$. $\langle f \rangle_{x}$
    denotes the average of $f$ over these uniformly sampled
    points. The law of large numbers ensures that the
    estimator $\langle I(f) \rangle_{x}$ will converge to
    the true value $I$ for $N \rightarrow \infty$. As a
    result of the central limit theorem, the function evaluations
    $f(\vec{x}_{i})$ will follow a Gaussian distribution with
    mean $\langle I(f) \rangle_{x}$,
    meaning an error esimate on the estimator is given by
    the standard error of the mean
    \begin{equation}\label{eqn:MC_error}
        \sigma_{I} = V \sqrt{\dfrac{\langle f^{2} \rangle_{x} - \langle f \rangle_{x}^{2}}{N}}
    \end{equation}
    which falls as $1/\sqrt{N}$, with no dependence
    on $d$.

    An algorithm that samples points $\vec{x}$ is called
    an integrator. To make use of the random numbers sampled,
    they need to be mapped to quantities that are useful in a simulation
    setting. The most commonly used mapping is to map
    the random numbers to four-momenta. An example of a
    simple integrator is the RAMBO algorithm \cite{Kleiss:1985gy,Platzer:2013esa}
    which samples four-momenta space uniformly. Since the integrator
    now effectively samples in momentum space, we refer to the points
    generated as phase-space points and so the terms integrator
    and phase-space generator are used interchangeably.

    Whilst Monte Carlo integration is independent of the
    dimensionality of the integral, meaning convergence is
    guaranteed for large enough $N$, there are variance
    reduction methods to more rapidly reduce the error
    on the estimator.

    One such method is importance sampling which
    aims to reduce the variance via a remapping of the
    uniformly sampled variables $\vec{x}$
    to a more suitable non-uniform distribution.
    In the context of particle physics,
    this corresponds to using a mapping of random numbers
    to four-momenta that captures the distribution of the
    matrix elements and their divergences, i.e. samples
    more frequently in regions where the matrix element
    is large in magnitude. Integrators inspired by the pole
    structure of multi-parton QCD processes have been
    studied in References \cite{Draggiotis:2000gm,vanHameren:2002tc}.
    
    Another common variance reduction technique is
    stratified sampling which divides the integration
    volume into sub-volumes, or bins. The overall integral
    and variances are the sums of the partial results
    in each bin. The overall variance is minimised when
    the the contributions from each bin are equal. This
    is achieved by sampling more from bins where the integral
    is rapidly fluctuating (and not necesarily large),
    and less from bins where the integral has less fluctuations.
    
    In practice, state-of-the-art event generators
    combine the concepts of importance sampling and stratified
    sampling with adaptive sampling \cite{Lepage:1977sw,Lepage:2020tgj}
    and multi-channelling methods \cite{Kleiss:1994qy,Ohl:1998jn}
    to achieve faster rates of convergence.

    \section{Event unweighting}\label{sec:unweighting}
    Once a suitable configuration of momenta
    has been found by the integrator, this point forms the basis of an
    event. The matrix element of interest can be evaluated at
    this phase-space point at the desired order in perturbation
    theory (see Section~\ref{sec:me_generators}). Additionally, there will be an associated Jacobian
    from the mapping from random variables to the four-momenta.
    This Jacobian is referred to as the phase-space weight, which together
    with the matrix element forms the event weight\footnote{For
    the more sophisticated sampling methods employed in event generators
    there will be additional weights involved.}
    \begin{equation}\label{eqn:event_weight}
        w_{i} = |\mathcal{M}_{i}|^{2} J_{i} \, .
    \end{equation}
    The event weight can be interpreted as the probability
    of this particular event occurring at an experiment \cite{Plehn:2009nd}.
    By generating a number of events in this manner,
    and histogramming them, a sample of weighted events is created.
    
    At experiments, measured events are either recorded
    or they are not, i.e. events come with unit-weight,
    not a variable weight as they are currently being
    generated with. In order to generate events that
    compare to reality, it has become customary to translate
    weighted events into unit-weight events.
    This process is known as unweighting and is typically
    carried out using a rejection sampling algorithm
    outlined in Algorithm~\ref{alg:unweighting}. The rejection
    sampling algorithm makes use of $w_{\mathrm{max}}$ which is the
    maximal event weight in the integration volume.

    \begin{algorithm}
        \caption{Rejection sampling for unweighting events}\label{alg:unweighting}
        \While{unweighting}{
            generate random phase-space point $\vec{x}$\;
            evaluate event weight $w \gets w(\vec{x})$\;
            generate uniform random number $r \gets$ Random(0, 1)\;
            \If{$w / w_{\mathrm{max}} > r$}{
                \KwRet $\vec{x}$ and $\tilde{w} \gets \max(1,\; w / w_{\mathrm{max}})$\;
            }
        }
    \end{algorithm}

    From this description, it is clear that the number
    of accepted events will generally be lower than
    the number of total events generated. This is encapsulated
    in the unweighting efficiency
    \begin{equation}\label{eqn:unweighting_efficiency}
        \varepsilon = \dfrac{N_{\mathrm{accepted}}}{N_{\mathrm{total}}} \, ,
    \end{equation}
    whose inverse is the average number of trial
    events required to obtain a single unweighted event.
    Unweighting efficiency for high-multiplicity processes
    are often well below 1\% \cite{Hoche:2019flt,Gao:2020zvv} and development
    of methods to improve the efficiency have been
    explored \cite{Jadach:1999sf,Jadach:2002kn}.
    These low efficiencies lead to most of the
    yearly CPU budgets of LHC experiments to be spent on
    unweighting events \cite{HSFPhysicsEventGeneratorWG:2020gxw}.

    An alternative to improving the unweighting efficiency
    itself is to accelerate the process of unweighting. This
    was explored in Reference \cite{Danziger:2021eeg} where a
    novel two-stage unweighting procedure employing a surrogate
    model of the exact event weight was used to decrease the amount of
    time spent unweighting events. This idea will be elaborated on
    in Chapter~\ref{chapter:fame3} where we build upon
    Reference \cite{Danziger:2021eeg} by increasing the fidelity of
    the surrogate model.

    \section{Anatomy of an event}\label{sec:anatomy_event}
    The simulation of an event can be split up into three
    main stages: the hard scattering process, followed by
    the parton shower, and finally hadronisation.
    This is motivated by the factorisation theorem
    which separates the high energy regimes from the low
    energy regimes.
    \subsection*{Hard scattering}\label{sec:me_generators}
    The first stage of an event simulation begins with
    the computation of the matrix element. The partonic
    channel of interest sets the multiplicity of the process,
    therefore informing the integrator on how to sample the phase-space
    accordingly.

    As seen in (\ref{eqn:matrix_element}),
    the exact matrix element is approximated by a perturbative
    series in a coupling constant. The calculations of
    tree-level \cite{Mangano:2002ea,Krauss:2001iv,Gleisberg:2008fv,Cafarella:2007pc,Alwall:2011uj,Kilian:2007gr}
    and one-loop \cite{Berger:2008sj,Cullen:2011ac,Cullen:2014yla,Bevilacqua:2011xh,Hirschi:2011pa,Alwall:2014hca,Frederix:2018nkq,Campbell:1999ah,Campbell:2021vlt,Badger:2012pg,Cascioli:2011va,Buccioni:2019sur,Actis:2016mpe,Denner:2017wsf}
    matrix elements are now largely fully automated
    in NLO QCD and electroweak corrections.
    Many of the modern matrix element generators are supplied
    with a common generic interface to event generators,
    the Binoth Les Houches Accord \cite{Binoth:2010xt,Alioli:2013nda}.
    For more information on the machinery developed to
    compute nearly arbitrary matrix elements at tree-level and
    one-loop level, see for instance Reference \cite{Campbell:2017hsr}.
    NNLO and N$^{3}$LO QCD corrections have been computed
    for select processes but have yet to be reach the
    level of automation of NLO.
    For a review of recent advancements see References \cite{Caola:2022ayt,Campbell:2022qmc}.
    
    Matrix element calculations make up a large
    portion of the total event generation time \cite{Bothmann:2022thx}.
    Reducing the time spent in this stage of the event
    generation therefore represents a sizeable increase
    in the number of events generated given a fixed computing
    budget \cite{HSFPhysicsEventGeneratorWG:2020gxw}.
    This thesis presents a novel technique to
    accelerate matrix element calculations by building
    accurate emulation models from modern machine learning
    techniques.

    \subsection*{Parton shower}\label{sec:parton_showers}
    Fixed-order calculations of matrix elements are limited
    to relatively few final-state particles due to the complexity
    of analytical expressions and are only accurate in the
    high energy limit. To evolve from the fixed-order
    calculation to the non-perturbative regime where hadrons
    are formed, parton showers are employed in event generators.

    Parton showers are implemented as algorithms that
    iteratively emit radiation (gluons and quarks in QCD
    parton showers) described by splitting functions.
    By repeating this procedure of emissions, parton
    showers produce a cascade of partons that naturally
    evolves the energy scale from the hard process to
    an infrared scale where non-perturbative effects
    begin to set in. In doing so parton showers account
    for potentially large logarithms that arise in
    these kinematic regimes at all orders in the coupling
    parameter, this is known as resummation \cite{Buckley:2011ms}.

    In event generators, fixed-order matrix elements
    and parton showers have to be utilised together to describe
    the many different observables at experiments. In order
    to combine the two stages of event generation in a consistent
    manner there are matching \cite{Frixione:2002ik,Frixione:2007vw,Jadach:2015mza}
    and merging \cite{Catani:2001cc,Lonnblad:2011xx}
    schemes.

    \subsection*{Hadronisation}\label{sec:hadronisation}
    The formation of hadrons in the low energy regime
    of an event generator follow non-perturbative models.
    The two main classes of models used to simulate
    hadronisation are the cluster models \cite{Webber:1983if,Winter:2003tt}
    and string models \cite{Andersson:1983ia}.
    These models share a similar concept of grouping together
    quarks and antiquarks to ensure only hadrons remain
    in the final-state. For more discussion on hadronisation
    see for example Reference \cite{Webber:1999ui}.

    \section{A note on theoretical uncertainties}\label{sec:scale_variations}
    The partonic cross-section, $\hat{\sigma}(\mu_{F}, \mu_{R})$ has
    a dependence on both the factorisation scale and the
    renormalisation scale. Although these scales are unphysical,
    and so any observable should not depend on them,
    there is a residual dependence on them in fixed-order
    calculations due to missing higher order terms.

    The most widely adopted approach to estimate the
    theoretical uncertainties associated with these
    missing higher order terms is to carry out a seven-point
    scale variation. Typically in a calculation we take
    $\mu = \mu_{F} = \mu_{R}$ where $\mu$ depends on the specifics
    of the process of interest. Taking the value of the
    partonic cross-section at this scale as the central value,
    the seven variations correspond to multiplying $\mu_{F}$
    and $\mu_{R}$ by the factors 
    \begin{equation}\label{eqn:scale_variations}
        \dfrac{\mu_{i}}{\mu} = \left\{(1, 1), (2,2),(0.5,0.5),(2,1),(1,2),(1,0.5),(0.5,1)\right\} \, ,
    \end{equation}
    and re-evaluating the
    cross-section at these scales.
    The estimate of the uncertainty is then taken
    to be the half-width of the maximum interval.


\end{document}